{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plm(last,prev):\n",
    "    l = 0\n",
    "    p = 0\n",
    "    file1 = open('count_2w.txt')\n",
    "    content = file1.read()\n",
    "    \n",
    "    tags = re.findall(r'\\b' + prev + ' ' + last + '.+',content)\n",
    "    \n",
    "    tags = re.findall(r'' + last + '\\s.+',content)\n",
    "    \n",
    "    for tag in tags:\n",
    "        array = (tag.split(','))\n",
    "        l += int(array[1])\n",
    "        \n",
    "    tags = re.findall(r'' + prev + '\\s.+',content)\n",
    "    \n",
    "    for tag in tags:\n",
    "        array = (tag.split(','))\n",
    "        p += int(array[1])\n",
    "        \n",
    "    l += 1\n",
    "    p += 333333\n",
    "\n",
    "    return (l*1.0/p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dairy==3.33967922594e-06\n",
      "in dairy life\n",
      "daisy==1.20977222468e-11\n",
      "in daisy life\n",
      "daily==0.0149957950453\n",
      "in daily life\n",
      "fairy==1.48238986531e-08\n",
      "in fairy life\n",
      "fail==1.92700246578e-08\n",
      "in fail life\n"
     ]
    }
   ],
   "source": [
    "def find(word):\n",
    "\n",
    "    arr = ['dairy','daisy','daily', 'fairy', 'fail']\n",
    "    size = len(word)\n",
    "    file2 = open('count_1w.txt')\n",
    "    content = file2.read()\n",
    "\n",
    "    \n",
    "    for w in arr:\n",
    "        wl = len(w)\n",
    "        table = []\n",
    "        row = []\n",
    "        for i in range(size + 1):\n",
    "            row.append(i)\n",
    "        table.append(row)\n",
    "        for i in range(1,wl + 1):\n",
    "            rows = []\n",
    "            rows.append(i)\n",
    "            table.append(rows)\n",
    "        for i in range(1,wl + 1):\n",
    "            for j in range(1,size + 1):\n",
    "                a = table[i-1][j]+1\n",
    "                b = table[i][j-1]+1\n",
    "                c = table[i-1][j-1] if word[j-1] == w[i-1] else table[i-1][j-1]+1\n",
    "                table[i].append(min(a,b,c))\n",
    "                \n",
    "        \n",
    "        distance = (table[len(table)-1][len(table[0])-1])\n",
    "\n",
    "        sentence = 'in ' + w + ' life'\n",
    "        line = sentence.split(' ')\n",
    "        multOfSent = 1.0\n",
    "\n",
    "        for i in range(len(line)-1,0,-1):\n",
    "            multOfSent *= plm(line[i],line[i-1])\n",
    "        ans = 1.0\n",
    "        \n",
    "        if distance == 0:\n",
    "            ans *= 0.95\n",
    "            \n",
    "        else:\n",
    "            ans *= (0.05 * math.pow(0.005,distance))\n",
    "        ans = ans * multOfSent\n",
    "        print(w + '==' + str(ans))\n",
    "        print(sentence)\n",
    "        \n",
    "find('daily')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/macpro/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"dataset.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['foo', 'bar', 'sentence']\n"
     ]
    }
   ],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "sentence = \"this is a foo bar sentence\"\n",
    "print [i for i in sentence.lower().split() if i not in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(data[\"news\"])\n",
    "y = list(data['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def clear_punctuation(s):\n",
    "    clear_string = \"\"\n",
    "    for symbol in s:\n",
    "        if symbol not in string.punctuation:\n",
    "            clear_string += symbol\n",
    "    return clear_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "clean = []\n",
    "for line in X:\n",
    "    line = clear_punctuation(line)\n",
    "    line = line.strip(\"/r/n\")\n",
    "    array = line.split(' ')\n",
    "    replaced = ''\n",
    "    for word in array:\n",
    "        if word.lower().strip() not in stopword:\n",
    "            replaced += word+' '\n",
    "    if replaced!='':\n",
    "        replaced=replaced[0:len(replaced)-1]\n",
    "        clean.append(index)\n",
    "    X[index] = replaced\n",
    "    index+=1\n",
    "X_new = []\n",
    "y_new = []\n",
    "for i in clean:\n",
    "    X_new.append(X[i])\n",
    "    y_new.append(y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'ascii' codec can't decode byte 0xc2 in position 0: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-f7a49e4afb2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mreplaced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mreplaced\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreplaced\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mreplaced\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreplaced\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplaced\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/nltk/stem/snowball.pyc\u001b[0m in \u001b[0;36mstem\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m   1425\u001b[0m         \u001b[0;31m# Map the different apostrophe characters to a single consistent one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m         word = (\n\u001b[0;32m-> 1427\u001b[0;31m             \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\u2019\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\x27\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1428\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\u2018\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\x27\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\u201B\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\x27\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'ascii' codec can't decode byte 0xc2 in position 0: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "for line in X_new:\n",
    "    array = line.split(' ')\n",
    "    replaced = ''\n",
    "    for word in array:\n",
    "        replaced += stemmer.stem(word)+' '\n",
    "    if replaced!='':\n",
    "        replaced=replaced[0:len(replaced)-1]\n",
    "    X_new[index] = replaced\n",
    "    index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'uk economi face major risks\\n \\n uk manufactur sector continu face serious challeng next two year british chamber commerc bcc said\\n \\n group quarter survey compani found export pick last three month 2004 best level eight year rise came despit exchang rate cite major concern howev bcc found whole uk economi still face major risk warn growth set slow recent forecast econom growth slow 3 2004 littl 25 2005 2006\\n \\n manufactur domest sale growth fell back slight quarter survey 5196 firm found employ manufactur also fell job expect lowest level year\\n \\n despit posit news export sector worri sign manufactur bcc said result reinforc concern sector persist inabl sustain recoveri outlook servic sector uncertain despit increas export order quarter bcc noted\\n \\n bcc found confid increas quarter across manufactur servic sector although overal fail reach level start 2004 reduc threat interest rate increas contribut improv confid said bank england rais interest rate five time novemb 2003 august last year rate kept hold sinc amid sign fall consum confid slowdown output pressur cost margin relentless increas regul threat higher tax remain serious problem bcc director general david frost said consum spend set deceler signific next 1218 month unlik invest export rise suffici strong pick slack\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
